# Retrieval-Augmented Generation (RAG) Project

With more widespread use of Large Language Models (LLMs), the risks of hallucination becomes significant. One approach to minimise hallucination is using [Retrieval-Augmented Generation](https://aws.amazon.com/what-is/retrieval-augmented-generation/) technique, where the LLM knowledge is bounded in a given context. Context comes from ingesting material (websites, documents, text, ...), creating embeddings and storing them in a vector database.

## Tech Stack
Backend: Python

Vectorstore: Pinecone
